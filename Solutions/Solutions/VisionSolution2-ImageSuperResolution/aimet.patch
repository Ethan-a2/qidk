diff --git a/zoo_torch/examples/superres/notebooks/superres_quanteval.ipynb b/zoo_torch/examples/superres/notebooks/superres_quanteval.ipynb
index b1ba5f6..05de5b7 100644
--- a/zoo_torch/examples/superres/notebooks/superres_quanteval.ipynb
+++ b/zoo_torch/examples/superres/notebooks/superres_quanteval.ipynb
@@ -14,7 +14,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 18,
    "id": "8181df87",
    "metadata": {
     "pycharm": {
@@ -25,6 +25,7 @@
    "source": [
     "import os, sys\n",
     "sys.path.append(os.path.dirname(os.getcwd()))\n",
+    "sys.path.insert(0,os.path.dirname(os.getcwd()))\n",
     "import glob\n",
     "import urllib.request\n",
     "import tarfile\n",
@@ -33,8 +34,8 @@
     "import numpy as np\n",
     "import torch\n",
     "import torch.nn as nn\n",
-    "from aimet_torch.quantsim import QuantizationSimModel\n",
-    "from aimet_torch.qc_quantize_op import QuantScheme\n",
+    "# from aimet_torch.quantsim import QuantizationSimModel\n",
+    "# from aimet_torch.qc_quantize_op import QuantScheme\n",
     "from utils.imresize import imresize\n",
     "from utils.models import *\n",
     "from utils.helpers import *\n",
@@ -54,6 +55,24 @@
     "# Global Constants"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 19,
+   "id": "ee0e2696-5f21-4be5-b61d-5bde253ab43b",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "['/local/mnt/workspace/shubgoya/exp_2.16/qidk-public-mirror/Solutions/VisionSolution2-ImageSuperResolution/aimet-model-zoo/zoo_torch/examples/superres', '/local/mnt/workspace/shubgoya/exp_2.16/qidk-public-mirror/Solutions/VisionSolution2-ImageSuperResolution/aimet-model-zoo/zoo_torch/examples/superres', '/local/mnt/workspace/shubgoya/exp_2.16/qidk-public-mirror/Solutions/VisionSolution2-ImageSuperResolution/aimet-model-zoo/zoo_torch/examples/superres/notebooks', '/local/mnt/workspace/snpe/2.16.0.231029/lib/python', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/local/mnt/workspace/aditya/env/lib/python3.8/site-packages', '/local/mnt/workspace/shubgoya/exp_2.16/qidk-public-mirror/Solutions/VisionSolution2-ImageSuperResolution/aimet-model-zoo/zoo_torch/examples/superres', '/local/mnt/workspace/shubgoya/exp_2.16/qidk-public-mirror/Solutions/VisionSolution2-ImageSuperResolution/aimet-model-zoo/zoo_torch/examples/superres']\n"
+     ]
+    }
+   ],
+   "source": [
+    "print(sys.path)"
+   ]
+  },
   {
    "cell_type": "markdown",
    "id": "5acd1f05",
@@ -64,13 +83,13 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 20,
    "id": "0e8a89a6",
    "metadata": {},
    "outputs": [],
    "source": [
     "''' Set the following variable to the path of your dataset (parent directory of actual images) '''\n",
-    "DATA_DIR = '/<path to parent>/set5/'\n",
+    "DATA_DIR = '/local/mnt/workspace/shubgoya/exp_2.16/qidk-public-mirror/Solutions/VisionSolution2-ImageSuperResolution/'\n",
     "DATASET_NAME = 'Set14' # Tested on Set5, Set14 and BSDS100\n",
     "\n",
     "# Directory to store downloaded checkpoints\n",
@@ -78,12 +97,12 @@
     "if not os.path.exists(CHECKPOINT_DIR):\n",
     "    os.makedirs(CHECKPOINT_DIR)\n",
     "    \n",
-    "use_cuda = True  # Whether to use CUDA or CPU"
+    "use_cuda = False  # Whether to use CUDA or CPU"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 21,
    "id": "c86b3209",
    "metadata": {
     "pycharm": {
@@ -202,10 +221,27 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 22,
    "id": "b33868e9",
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "{0: 'ABPNRelease',\n",
+       " 1: 'XLSRRelease',\n",
+       " 2: 'SESRRelease_M3',\n",
+       " 3: 'SESRRelease_M5',\n",
+       " 4: 'SESRRelease_M7',\n",
+       " 5: 'SESRRelease_M11',\n",
+       " 6: 'SESRRelease_XL'}"
+      ]
+     },
+     "execution_count": 22,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "MODEL_DICT = {}\n",
     "for idx in range(len(MODELS)):\n",
@@ -224,21 +260,32 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 23,
    "id": "d479cd0e",
    "metadata": {},
    "outputs": [],
    "source": [
     "''' Set this variable'''\n",
-    "model_index = 0  # Model index"
+    "model_index = 6  # Model index"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 24,
    "id": "d9c7fd63",
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "{0: 'sesr_xl_2x', 1: 'sesr_xl_3x', 2: 'sesr_xl_4x'}"
+      ]
+     },
+     "execution_count": 24,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "MODEL_NAME = MODELS[model_index]  # Selected model type\n",
     "\n",
@@ -261,7 +308,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 25,
    "id": "5388b4b4",
    "metadata": {},
    "outputs": [],
@@ -272,10 +319,18 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 26,
    "id": "7e95616f",
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "sesr_xl_2x will be used\n"
+     ]
+    }
+   ],
    "source": [
     "# Choose model\n",
     "MODEL_CONFIG = MODEL_SPECS[model_spec_index]\n",
@@ -292,7 +347,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 27,
    "id": "30a5137f",
    "metadata": {},
    "outputs": [],
@@ -313,10 +368,18 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 11,
    "id": "4fae0a27",
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Downloading model weights\n"
+     ]
+    }
+   ],
    "source": [
     "if not os.path.exists(MODEL_PATH_INT8) or os.path.exists(MODEL_PATH_FP32) or os.path.exists(ENCODING_PATH):\n",
     "    print('Downloading model weights')\n",
@@ -345,16 +408,52 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 34,
    "id": "1c392f35",
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n",
+      "(128, 128, 3)\n"
+     ]
+    }
+   ],
    "source": [
     "# Path to test images\n",
     "TEST_IMAGES_DIR = os.path.join(DATA_DIR, DATASET_NAME)\n",
     "\n",
     "# Get test images\n",
-    "INPUTS_LR, IMAGES_LR, IMAGES_HR = load_dataset(TEST_IMAGES_DIR, MODEL_ARGS[MODEL_NAME].get(MODEL_CONFIG)['scaling_factor'])"
+    "INPUTS_LR, IMAGES_LR, IMAGES_HR = load_dataset(TEST_IMAGES_DIR, MODEL_ARGS[MODEL_NAME].get(MODEL_CONFIG)['scaling_factor'])\n",
+    "\n",
+    "!mkdir -p raw\n",
+    "for i, img_path in enumerate(glob.glob(os.path.join(TEST_IMAGES_DIR, '*'))):\n",
+    "    img = cv2.imread(img_path)\n",
+    "    img = cv2.resize(img,[128,128],interpolation=cv2.INTER_CUBIC)\n",
+    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
+    "    img = img/255\n",
+    "    print(img.shape)\n",
+    "        # img_lr = cv2.resize(img_lr,[128,128],interpolation=cv2.INTER_CUBIC)\n",
+    "    \n",
+    "        # print(img_lr.shape)\n",
+    "    img = img.astype(np.float32)\n",
+    "    fid = open(\"raw/img\"+str(i)+ \".raw\", 'wb')\n",
+    "    img.tofile(fid)\n",
+    "    fid.close()\n"
    ]
   },
   {
@@ -371,7 +470,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 29,
    "id": "aca47bf9",
    "metadata": {
     "pycharm": {
@@ -379,24 +478,32 @@
     },
     "scrolled": true
    },
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Loading model from checkpoint : ./checkpoints/release_sesr_xl_2x/checkpoint_float32.pth.tar\n"
+     ]
+    }
+   ],
    "source": [
     "# Load the model\n",
     "model_original_fp32 = load_model(MODEL_PATH_FP32, MODEL_NAME, MODEL_ARGS[MODEL_NAME].get(MODEL_CONFIG), \n",
     "                   use_quant_sim_model=False, encoding_path=None, \n",
     "                   calibration_data=None, use_cuda=use_cuda)\n",
     "\n",
-    "model_original_int8 = load_model(MODEL_PATH_FP32, MODEL_NAME, MODEL_ARGS[MODEL_NAME].get(MODEL_CONFIG), \n",
-    "                   use_quant_sim_model=True, encoding_path=None, \n",
-    "                   calibration_data=IMAGES_HR, use_cuda=use_cuda)\n",
+    "# model_original_int8 = load_model(MODEL_PATH_FP32, MODEL_NAME, MODEL_ARGS[MODEL_NAME].get(MODEL_CONFIG), \n",
+    "#                    use_quant_sim_model=True, encoding_path=None, \n",
+    "#                    calibration_data=IMAGES_HR, use_cuda=use_cuda)\n",
     "\n",
-    "model_optimized_fp32 = load_model(MODEL_PATH_INT8, MODEL_NAME, MODEL_ARGS[MODEL_NAME].get(MODEL_CONFIG), \n",
-    "                   use_quant_sim_model=False, encoding_path=None, \n",
-    "                   calibration_data=None, use_cuda=use_cuda)\n",
+    "# model_optimized_fp32 = load_model(MODEL_PATH_INT8, MODEL_NAME, MODEL_ARGS[MODEL_NAME].get(MODEL_CONFIG), \n",
+    "#                    use_quant_sim_model=False, encoding_path=None, \n",
+    "#                    calibration_data=None, use_cuda=use_cuda)\n",
     "\n",
-    "model_optimized_int8 = load_model(MODEL_PATH_INT8, MODEL_NAME, MODEL_ARGS[MODEL_NAME].get(MODEL_CONFIG), \n",
-    "                   use_quant_sim_model=True, encoding_path=ENCODING_PATH, \n",
-    "                   calibration_data=IMAGES_HR, use_cuda=use_cuda)"
+    "# model_optimized_int8 = load_model(MODEL_PATH_INT8, MODEL_NAME, MODEL_ARGS[MODEL_NAME].get(MODEL_CONFIG), \n",
+    "#                    use_quant_sim_model=True, encoding_path=ENCODING_PATH, \n",
+    "#                    calibration_data=IMAGES_HR, use_cuda=use_cuda)"
    ]
   },
   {
@@ -417,16 +524,24 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 30,
    "id": "8819afb6",
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\n"
+     ]
+    }
+   ],
    "source": [
     "# Run model inference on test images and get super-resolved images\n",
     "IMAGES_SR_original_fp32 = run_model(model_original_fp32, INPUTS_LR, use_cuda)\n",
-    "IMAGES_SR_original_int8 = run_model(model_original_int8, INPUTS_LR, use_cuda)\n",
-    "IMAGES_SR_optimized_fp32 = run_model(model_optimized_fp32, INPUTS_LR, use_cuda)\n",
-    "IMAGES_SR_optimized_int8 = run_model(model_optimized_int8, INPUTS_LR, use_cuda)"
+    "# IMAGES_SR_original_int8 = run_model(model_original_int8, INPUTS_LR, use_cuda)\n",
+    "# IMAGES_SR_optimized_fp32 = run_model(model_optimized_fp32, INPUTS_LR, use_cuda)\n",
+    "# IMAGES_SR_optimized_int8 = run_model(model_optimized_int8, INPUTS_LR, use_cuda)"
    ]
   },
   {
@@ -439,21 +554,161 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 51,
    "id": "19f63d02",
    "metadata": {},
    "outputs": [],
    "source": [
-    "# Get the average PSNR for all test-images\n",
-    "avg_psnr = evaluate_average_psnr(IMAGES_SR_original_fp32, IMAGES_HR)\n",
-    "print(f'Original Model | FP32 Environment | Avg. PSNR: {avg_psnr:.3f}')\n",
-    "avg_psnr = evaluate_average_psnr(IMAGES_SR_original_int8, IMAGES_HR)\n",
-    "print(f'Original Model | INT8 Environment | Avg. PSNR: {avg_psnr:.3f}')\n",
-    "avg_psnr = evaluate_average_psnr(IMAGES_SR_optimized_fp32, IMAGES_HR)\n",
-    "print(f'Optimized Model | FP32 Environment | Avg. PSNR: {avg_psnr:.3f}')\n",
-    "avg_psnr = evaluate_average_psnr(IMAGES_SR_optimized_int8, IMAGES_HR)\n",
-    "print(f'Optimized Model | INT8 Environment | Avg. PSNR: {avg_psnr:.3f}')"
+    "# # Get the average PSNR for all test-images\n",
+    "# avg_psnr = evaluate_average_psnr(IMAGES_SR_original_fp32, IMAGES_HR)\n",
+    "# print(f'Original Model | FP32 Environment | Avg. PSNR: {avg_psnr:.3f}')\n",
+    "# avg_psnr = evaluate_average_psnr(IMAGES_SR_original_int8, IMAGES_HR)\n",
+    "# print(f'Original Model | INT8 Environment | Avg. PSNR: {avg_psnr:.3f}')\n",
+    "# avg_psnr = evaluate_average_psnr(IMAGES_SR_optimized_fp32, IMAGES_HR)\n",
+    "# print(f'Optimized Model | FP32 Environment | Avg. PSNR: {avg_psnr:.3f}')\n",
+    "# avg_psnr = evaluate_average_psnr(IMAGES_SR_optimized_int8, IMAGES_HR)\n",
+    "# print(f'Optimized Model | INT8 Environment | Avg. PSNR: {avg_psnr:.3f}')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 31,
+   "id": "7e1f9e62-7455-4b5f-a649-563bf3107499",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "2023-11-24 23:27:35,910 - 235 - INFO - Successfully simplified the onnx model in child process\n",
+      "2023-11-24 23:27:35,932 - 235 - INFO - Successfully receive the simplified onnx model in main process\n",
+      "2023-11-24 23:27:35,942 - 235 - INFO - Successfully run shape inference in child process\n",
+      "2023-11-24 23:27:35,959 - 235 - INFO - Successfully receive the inferred model in main process\n",
+      "2023-11-24 23:27:36,070 - 235 - INFO - INFO_INITIALIZATION_SUCCESS: \n",
+      "2023-11-24 23:27:36,082 - 235 - INFO - INFO_CONVERSION_SUCCESS: Conversion completed successfully\n",
+      "2023-11-24 23:27:36,089 - 235 - INFO - INFO_WRITE_SUCCESS: \n"
+     ]
+    }
+   ],
+   "source": [
+    "%%bash\n",
+    "snpe-onnx-to-dlc --input_network super_resolution.onnx --output_path super_resolution_sesr_opt.dlc"
    ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 35,
+   "id": "00a5af54-fa16-4b53-9fa4-82abd9411503",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "%%bash\n",
+    "find -name *.raw >input.txt"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 36,
+   "id": "8b57dcc2-afc0-4bc7-868d-ed2755d649e8",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "./raw/img1.raw\n",
+      "./raw/img7.raw\n",
+      "./raw/img13.raw\n",
+      "./raw/img0.raw\n",
+      "./raw/img5.raw\n",
+      "./raw/img4.raw\n",
+      "./raw/img10.raw\n",
+      "./raw/img2.raw\n",
+      "./raw/img11.raw\n",
+      "./raw/img6.raw\n",
+      "./raw/img12.raw\n",
+      "./raw/img3.raw\n",
+      "./raw/img8.raw\n",
+      "./raw/img9.raw\n"
+     ]
+    }
+   ],
+   "source": [
+    "%%bash\n",
+    "cat input.txt"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 37,
+   "id": "29b9a859-b160-4572-bf20-f1be52d42b77",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "[INFO] InitializeStderr: DebugLog initialized.\n",
+      "[INFO] Processed command-line arguments\n",
+      "[INFO] Quantized parameters\n",
+      "[INFO] Generated activations\n",
+      "[INFO] Saved quantized dlc to: ../../../../../superresolution/src/main/assets/Quant_SuperResolution_sesr.dlc\n",
+      "[INFO] DebugLog shutting down.\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "     0.0ms [  INFO ] Inferences will run in sync mode\n",
+      "     0.1ms [  INFO ] Initializing logging in the backend. Callback: [0xcf3a80], Log Level: [3]\n",
+      "     0.1ms [  INFO ] No BackendExtensions lib provided;initializing NetRunBackend Interface\n",
+      "     0.1ms [WARNING] Unable to find a device with NetRunDeviceKeyDefault in Library NetRunBackendLibKeyDefault\n",
+      "     0.1ms [  INFO ] Entering QuantizeRuntimeApp flow\n",
+      "   484.9ms [  INFO ] cleaning up resources for input tensors\n",
+      "   484.9ms [  INFO ] cleaning up resources for output tensors\n",
+      "   821.4ms [  INFO ] cleaning up resources for input tensors\n",
+      "   821.5ms [  INFO ] cleaning up resources for output tensors\n",
+      "  1154.6ms [  INFO ] cleaning up resources for input tensors\n",
+      "  1154.6ms [  INFO ] cleaning up resources for output tensors\n",
+      "  1491.7ms [  INFO ] cleaning up resources for input tensors\n",
+      "  1491.7ms [  INFO ] cleaning up resources for output tensors\n",
+      "  1829.5ms [  INFO ] cleaning up resources for input tensors\n",
+      "  1829.5ms [  INFO ] cleaning up resources for output tensors\n",
+      "  2158.0ms [  INFO ] cleaning up resources for input tensors\n",
+      "  2158.0ms [  INFO ] cleaning up resources for output tensors\n",
+      "  2500.1ms [  INFO ] cleaning up resources for input tensors\n",
+      "  2500.1ms [  INFO ] cleaning up resources for output tensors\n",
+      "  2840.4ms [  INFO ] cleaning up resources for input tensors\n",
+      "  2840.4ms [  INFO ] cleaning up resources for output tensors\n",
+      "  3171.0ms [  INFO ] cleaning up resources for input tensors\n",
+      "  3171.0ms [  INFO ] cleaning up resources for output tensors\n",
+      "  3511.2ms [  INFO ] cleaning up resources for input tensors\n",
+      "  3511.2ms [  INFO ] cleaning up resources for output tensors\n",
+      "  3841.4ms [  INFO ] cleaning up resources for input tensors\n",
+      "  3841.4ms [  INFO ] cleaning up resources for output tensors\n",
+      "  4176.5ms [  INFO ] cleaning up resources for input tensors\n",
+      "  4176.5ms [  INFO ] cleaning up resources for output tensors\n",
+      "  4514.5ms [  INFO ] cleaning up resources for input tensors\n",
+      "  4514.5ms [  INFO ] cleaning up resources for output tensors\n",
+      "  4844.9ms [  INFO ] cleaning up resources for input tensors\n",
+      "  4844.9ms [  INFO ] cleaning up resources for output tensors\n",
+      "  5013.2ms [  INFO ] Freeing graphsInfo\n"
+     ]
+    }
+   ],
+   "source": [
+    "%%bash\n",
+    "snpe-dlc-quantize --input_dlc super_resolution_sesr_opt.dlc --input_list input.txt --use_enhanced_quantizer --use_adjusted_weights_quantizer --axis_quant --output_dlc ../../../../../superresolution/src/main/assets/Quant_SuperResolution_sesr.dlc"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "bfc6fc69-290e-4dbd-b175-2268302a9e83",
+   "metadata": {},
+   "outputs": [],
+   "source": []
   }
  ],
  "metadata": {
@@ -472,7 +727,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.0"
+   "version": "3.8.17"
   }
  },
  "nbformat": 4,
diff --git a/zoo_torch/examples/superres/utils/imresize.py b/zoo_torch/examples/superres/utils/imresize.py
index 253595f..4100b59 100644
--- a/zoo_torch/examples/superres/utils/imresize.py
+++ b/zoo_torch/examples/superres/utils/imresize.py
@@ -128,9 +128,9 @@ def resizeAlongDim(A, dim, weights, indices, mode="vec"):
 
 
 def imresize(I, scalar_scale=None, method='bicubic', output_shape=None, mode="vec"):
-    if method is 'bicubic':
+    if method == 'bicubic':
         kernel = cubic
-    elif method is 'bilinear':
+    elif method == 'bilinear':
         kernel = triangle
     else:
         print('Error: Unidentified method supplied')
diff --git a/zoo_torch/examples/superres/utils/inference.py b/zoo_torch/examples/superres/utils/inference.py
index 6bc22b8..f74fd46 100644
--- a/zoo_torch/examples/superres/utils/inference.py
+++ b/zoo_torch/examples/superres/utils/inference.py
@@ -10,8 +10,8 @@
 
 import torch
 import torch.nn as nn
-from aimet_torch.quantsim import QuantizationSimModel
-from aimet_torch.qc_quantize_op import QuantScheme
+# from aimet_torch.quantsim import QuantizationSimModel
+# from aimet_torch.qc_quantize_op import QuantScheme
 from .models import *
 from .helpers import pass_calibration_data, post_process
 
@@ -100,8 +100,12 @@ def run_model(model, inputs_lr, use_cuda):
     for count, img_lr in enumerate(inputs_lr):
         with torch.no_grad():
             sr_img = model(img_lr.unsqueeze(0).to(device)).squeeze(0)
-
+            input_shape = [1, 3, 128, 128]
+            input_data = torch.randn(input_shape)
+            torch.onnx.export(model, input_data, "super_resolution.onnx", export_params=True,
+                          opset_version=11, do_constant_folding=True, input_names = ['lr'], output_names = ['sr'])
         images_sr.append(post_process(sr_img))
+
     print('')
 
     return images_sr
\ No newline at end of file
