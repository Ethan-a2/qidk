{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e07403-f9fe-445b-80b5-8e627885c0f0",
   "metadata": {},
   "source": [
    "# DLC Generation\n",
    "- Model used [MobileBert_Paper](https://arxiv.org/pdf/2004.02984.pdf) , [Huggingface Link](https://huggingface.co/Alireza1044/mobilebert_sst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140fff04-0721-4a66-9f48-292262ffb283",
   "metadata": {},
   "source": [
    "### Taking the Model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471752aa-a3b9-4d52-b105-165a3fb63c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 11:34:04.908118: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-12 11:34:04.935372: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 11:34:05.424667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-03-12 11:34:09.271374: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMobileBertForSequenceClassification: ['mobilebert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFMobileBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMobileBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFMobileBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMobileBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from transformers import TensorType\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import sys\n",
    "\n",
    "bs = 1\n",
    "SEQ_LEN = 128\n",
    "MODEL_NAME = \"Alireza1044/mobilebert_sst2\"\n",
    "\n",
    "# Allocate tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME, from_pt=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caa9d3-499d-4679-9725-30cef3dbe90b",
   "metadata": {},
   "source": [
    "### Converting the Model to Tensorflow keras format(.pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33092fe-fe95-41bd-998c-f502166ee2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(input_ids, attention_mask):\n",
    "    output = tf.nn.softmax(model(input_ids, attention_mask).logits, axis=-1)\n",
    "    return output\n",
    "\n",
    "model_fn = tf.function(\n",
    "    model_fn,\n",
    "    input_signature=[\n",
    "        tf.TensorSpec(shape=[bs, SEQ_LEN], dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=[bs, SEQ_LEN], dtype=tf.int32)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c9dcc6-5715-453e-b6cf-d4df30139aaf",
   "metadata": {},
   "source": [
    "#### Checking the Tensorflow Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb74ca2a-52c3-4cb2-9df9-5236d2091387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context = \n",
      "It is easy to say but hard to do ...\n",
      "\n",
      "Prediction: 0.02% positive & 99.98% negative\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter to continue ... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sample input\n",
    "context = \"It is easy to say but hard to do ...\"\n",
    "\n",
    "input_encodings = tokenizer(\n",
    "            context,\n",
    "            return_tensors=TensorType.TENSORFLOW,\n",
    "            # return_tensors=\"np\",\n",
    "            padding='max_length',\n",
    "            return_length=True,\n",
    "            max_length=SEQ_LEN,\n",
    "            return_special_tokens_mask=True\n",
    "        )\n",
    "# print(input_encodings)\n",
    "\n",
    "print(f\"\\nContext = \\n{context}\")\n",
    "logits = model_fn(input_encodings.input_ids, input_encodings.attention_mask)\n",
    "# print(logits)\n",
    "# print(logits.shape)\n",
    "\n",
    "positivity = logits[0][1] * 100\n",
    "negativity = logits[0][0] * 100\n",
    "\n",
    "print(f\"\\nPrediction: {positivity:.2f}% positive & {negativity:.2f}% negative\\n\")\n",
    "\n",
    "input(\"Enter to continue ...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94344cf5-e04c-462a-b91f-2d3ad1b0ac15",
   "metadata": {},
   "source": [
    "#### Saving the tensorflow model to .pb format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f37cccd9-f170-46e2-8b7b-3362e8128fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 11:39:55.460568: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-12 11:39:55.460670: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "NO. of Frozen model layers: 5747\n",
      "--------------------------------------------------\n",
      "Frozen model inputs: \n",
      "[<tf.Tensor 'input_ids:0' shape=(1, 128) dtype=int32>, <tf.Tensor 'attention_mask:0' shape=(1, 128) dtype=int32>]\n",
      "Frozen model outputs: \n",
      "[<tf.Tensor 'Identity:0' shape=(1, 2) dtype=float32>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./frozen_models/mobilebert_sst2.pb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "frozen_func = convert_variables_to_constants_v2(model_fn.get_concrete_function())\n",
    "\n",
    "layers = [op.name for op in frozen_func.graph.get_operations()]\n",
    "print(\"-\" * 50)\n",
    "print(\"NO. of Frozen model layers: {}\".format(len(layers)))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Frozen model inputs: \")\n",
    "print(frozen_func.inputs)\n",
    "print(\"Frozen model outputs: \")\n",
    "print(frozen_func.outputs)\n",
    "\n",
    "graph_def = frozen_func.graph.as_graph_def()\n",
    "\n",
    "graph_def = tf.compat.v1.graph_util.remove_training_nodes(graph_def)\n",
    "\n",
    "tf.io.write_graph(graph_or_graph_def=graph_def,\n",
    "                  logdir=\"./frozen_models\",\n",
    "                  name=\"mobilebert_sst2.pb\",\n",
    "                  as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c995cc2-dca3-4620-bbcb-2952ae6cb956",
   "metadata": {},
   "source": [
    "## Converting the Model to DLC Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757fa4cd-4df7-4196-826a-e97eb5686220",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Give appropriate SNPE-ROOT\n",
    "import os\n",
    "os.environ['SNPE_ROOT']=\"/local/mnt/workspace/snpe/snpe-2.20/2.20.0.240223/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa60295-3590-401d-a6a3-a79ef19868ce",
   "metadata": {},
   "source": [
    "##### Converting the Model to FP-32 Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b15511-d6f3-4d4a-9a65-8d0058f21bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[INFO] AISW SDK environment set\n",
      "[INFO] SNPE_ROOT: /local/mnt/workspace/snpe/snpe-2.20/2.20.0.240223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 11:41:19.603667: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-12 11:41:19.629782: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 11:41:20.124698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-03-12 11:41:20.834829: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-03-12 11:41:23.478095: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2024-03-12 11:41:26,990 - 235 - INFO - INFO_ALL_BUILDING_NETWORK: \n",
      "    ==============================================================\n",
      "    Building Network\n",
      "    ==============================================================\n",
      "2024-03-12 11:41:27,547 - 235 - INFO - Resolving static sub-graphs in network...\n",
      "2024-03-12 11:41:27,631 - 235 - INFO - Resolving static sub-graphs in network, complete.\n",
      "2024-03-12 11:41:37,655 - 235 - INFO - INFO_INITIALIZATION_SUCCESS: \n",
      "2024-03-12 11:41:38,075 - 235 - INFO - INFO_CONVERSION_SUCCESS: Conversion completed successfully\n",
      "2024-03-12 11:41:38,280 - 235 - INFO - INFO_WRITE_SUCCESS: \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source $SNPE_ROOT/bin/envsetup.sh\n",
    "snpe-tensorflow-to-dlc -i frozen_models/mobilebert_sst2.pb -d input_ids 1,128 -d attention_mask 1,128 --out_node Identity -o frozen_models/mobilebert_sst2.dlc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e327a-78c8-46d1-9244-d4a9f3444c9e",
   "metadata": {},
   "source": [
    "##### Converting the Model to FP16 Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60336879-406d-4d27-97fa-7338fd3efa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[INFO] AISW SDK environment set\n",
      "[INFO] SNPE_ROOT: /local/mnt/workspace/snpe/snpe-2.20/2.20.0.240223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] InitializeStderr: DebugLog initialized.\n",
      "[INFO] SNPE HTP Offline Prepare: Attempting to create cache for SM8650\n",
      "[USER_INFO] Target device backend record identifier: HTP_V75_8MB\n",
      "[USER_INFO] No cache record in the DLC matches the target device (HTP_V75_8MB). Creating a new record\n",
      "[INFO] Attempting to open dynamically linked lib: libHtpPrepare.so\n",
      "[INFO] dlopen libHtpPrepare.so SUCCESS handle 0x24298d0\n",
      "[INFO] Found Interface Provider (v2.14)\n",
      "[USER_WARNING] QnnDsp <W> Initializing HtpProvider\n",
      "[USER_WARNING] QnnDsp <W> HTP arch will be deprecated, please set SoC id instead.\n",
      "[USER_WARNING] QnnDsp <W> Performance Estimates unsupported\n",
      "[USER_INFO] Platform option not set\n",
      "[USER_INFO] Created ctx=0x1 for Snpe Unique Graph ID=0 backend=3 instancePtr=0x24294d8\n",
      "[USER_INFO] FP16 precision enabled for graph with id=0\n",
      "[USER_INFO] Offline Prepare VTCM size(MB) selected = 8\n",
      "[USER_INFO] Offline Prepare Optimization Level passed = 2\n",
      "[USER_INFO] Backend Mgr ~Dtor called for backend HTP\n",
      "[USER_INFO] Cleaning up Context=0x1 for Snpe Unique Graph ID=0 backend=3 instancePtr=0x24294d8\n",
      "[USER_INFO] DONE Cleaning up Context=0x1 for Snpe Unique Graph ID=0 backend=3 instancePtr=0x24294d8\n",
      "[USER_INFO] BackendTerminate triggered\n",
      "[INFO] SNPE HTP Offline Prepare: Successfully created cache for SM8650\n",
      "[INFO] ======== Run Summary ========\n",
      "[INFO]   SM8650 :  Success\n",
      "[USER_INFO] BackendTerminate triggered\n",
      "[INFO] DebugLog shutting down.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source $SNPE_ROOT/bin/envsetup.sh\n",
    "snpe-dlc-graph-prepare --input_dlc frozen_models/mobilebert_sst2.dlc --use_float_io --htp_archs v75 --set_output_tensors Identity:0,Identity_1:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135df7f-60e0-4c9f-a797-74f391c27e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
